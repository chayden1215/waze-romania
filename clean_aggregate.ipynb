{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569c82e6",
   "metadata": {},
   "source": [
    "# Cleaning and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a603e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b794f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = '/Users/catherinehayden/WB/cluj' # desired location on local disk\n",
    "\n",
    "# read data into Dask\n",
    "ddf = dd.read_csv(\n",
    "        \"RO_9clujnapoca_alerts.000000000000.csv.gz\", \n",
    "        compression=\"gzip\", \n",
    "        blocksize=None, \n",
    "        dtype={\n",
    "            'confidence': 'int8',\n",
    "            'type': 'category',\n",
    "            'subtype': 'category',\n",
    "            'roadType': 'float32', # to accept NA values\n",
    "            'reliability': 'int8',\n",
    "            'magvar': 'int16',\n",
    "            'street': 'object'\n",
    "        },\n",
    ")\n",
    "\n",
    "\n",
    "ddf[\"ts\"] = dd.to_datetime(ddf[\"ts\"], utc=True).dt.tz_convert('Europe/Bucharest')\n",
    "# Note: after changing timezone to bucharest, we have 2 observations taking place on jan 1 2022\n",
    "ddf[\"date\"] = ddf[\"ts\"].dt.date\n",
    "ddf[\"dayofweek\"] = ddf[\"ts\"].dt.dayofweek # monday = 0, sunday = 6\n",
    "ddf['year'] = ddf['ts'].dt.year\n",
    "ddf['month'] = ddf['ts'].dt.month\n",
    "ddf['hour'] = ddf['ts'].dt.hour\n",
    "ddf['timeofday'] = 1 # to be set to Nielsen Audio dayparting times\n",
    "ddf['timeofday'] = ((ddf.hour>5)& (ddf.hour<10))*1 + ((ddf.hour>9) & (ddf.hour<16))*2 + \\\n",
    "((ddf.hour>15) & (ddf.hour<20))*3 + ((ddf.hour>19) & (ddf.hour<=23))*4 + ((ddf.hour>=0) & (ddf.hour<6))*5\n",
    "\n",
    "# computations with geopandas and pandas\n",
    "df = ddf.compute()\n",
    "\n",
    "geocsv = df.to_csv(outputpath + \"/geodata.csv\", index = False)\n",
    "\n",
    "df['coordinates'] = df['geoWKT'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='coordinates')\n",
    "df['lon'] = gdf.geometry.x\n",
    "df['lat'] = gdf.geometry.y\n",
    "df = df.drop(['coordinates'], axis=1)\n",
    "\n",
    "# adding in rush hour times sent by our Romanian colleagues\n",
    "def rush(series): \n",
    "    if (series>=7) & (series<=9):\n",
    "        return \"Morning Rush\"\n",
    "    elif (series>=16) & (series<=19):\n",
    "        return \"Afternoon Rush\"\n",
    "    else:\n",
    "        return \"No Rush\"\n",
    "df['rush'] = df['hour'].apply(rush)\n",
    "\n",
    "\n",
    "# mapping obs to h3 hexagons\n",
    "df[\"h3\"] = df.apply(lambda x: h3.geo_to_h3(x[\"lat\"], x[\"lon\"], 10), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6eeaf",
   "metadata": {},
   "source": [
    "## Exporting full cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc150c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(outputpath + \"/fullclean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e392f",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9aa795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by date\n",
    "datehex = df.groupby([\"date\", \"h3\"], as_index = False)[\"uuid\"].count()\n",
    "\n",
    "datehex = datehex.rename(columns = {'uuid': 'AlertCounts'})\n",
    "\n",
    "datehex.to_csv(outputpath + \"/datehex.csv\", index = False)\n",
    "\n",
    "# by day of the week\n",
    "weekdayhextype = df.groupby([\"dayofweek\", \"h3\", \"type\"], as_index = False)[\"uuid\"].count()\n",
    "\n",
    "weekdayhextype = weekdayhextype.rename(columns = {'uuid': 'AlertCount'})\n",
    "\n",
    "weekdayhextype.to_csv(outputpath + \"/weekdayhextype.csv\", index = False)\n",
    "\n",
    "\n",
    "weekdayhex = df.groupby([\"dayofweek\", \"h3\"], as_index = False)[\"uuid\"].count()\n",
    "\n",
    "weekdayhex = weekdayhex.rename(columns = {'uuid': 'AlertCount'})\n",
    "\n",
    "weekdayhex.to_csv(outputpath + \"/weekdayhex.csv\", index = False)\n",
    "\n",
    "# by hour of the day\n",
    "hourhextype = df.groupby([\"hour\", \"h3\", \"type\"], as_index = False)[\"uuid\"].count()\n",
    "\n",
    "hourhextype = hourhextype.rename(columns = {'uuid': 'AlertCount'})\n",
    "\n",
    "hourhextype = hourhextype[hourhextype.AlertCount > 0]\n",
    "\n",
    "hourhextype.to_csv(outputpath + \"/hourhextype.csv\", index = False)\n",
    "\n",
    "\n",
    "hourhex = df.groupby([\"hour\", \"h3\"], as_index = False)[\"uuid\"].count()\n",
    "\n",
    "hourhex = hourhex.rename(columns = {'uuid': 'AlertCount'})\n",
    "\n",
    "hourhex.to_csv(outputpath + \"/hourhex.csv\", index = False)\n",
    "\n",
    "# by rush hour designation\n",
    "rushhex = df.groupby([\"rush\", \"h3\"], as_index = False)[\"uuid\"].count()\n",
    "\n",
    "rushhex = rushhex.rename(columns = {'uuid': 'AlertCount'})\n",
    "\n",
    "rushhex.to_csv(outputpath + \"/rushhex.csv\", index = False)\n",
    "\n",
    "\n",
    "rushhexweekday = df.groupby([\"rush\", \"h3\", \"dayofweek\"], as_index = False)[\"uuid\"].count()\n",
    "\n",
    "rushhexweekday = rushhexweekday.rename(columns = {'uuid': 'AlertCount'})\n",
    "\n",
    "rushhexweekday.to_csv(outputpath + \"/rushhexweekday.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
